{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "#MODEL = 'ILSVRC' # ImageNet, don't use ImageNet, it wasn't trained on all categories\n",
    "#MODEL = 'coco' # MS-Coco\n",
    "MODEL = 'ours'\n",
    "IMAGE_SIZE = 300 # 300x300 trained on coco or ILSVRC \n",
    "# I wonder if we can take the coco model and further train it on\n",
    "# http://image-net.org/synset?wnid=n02773838\n",
    "#IMAGE_SIZE = 512 # for 512x512 trained on coco\n",
    "THRESHOLD = 0.20 # for detection - percentage that the model is sure it's what you're looking for\n",
    "# There are 21 categories.... pick one color for each\n",
    "# just a tool for label finding\n",
    "any_in = lambda a, b: bool(set(a).intersection(b)) #for checking if a list contains elements of another\n",
    "COLORS = plt.cm.hsv(np.linspace(0, 1, 255)).tolist() #for picking colors of the boxes\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "#caffe.set_mode_cpu()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "# load COCO labels\n",
    "if MODEL == 'ours':\n",
    "    labelmap_file = 'data/coco/labelmap_coco.prototxt'\n",
    "if MODEL == 'coco':\n",
    "    labelmap_file = 'data/coco/labelmap_coco2.prototxt'\n",
    "else:\n",
    "    labelmap_file = 'data/ILSVRC2016/labelmap_ilsvrc_det.prototxt'\n",
    "file = open(labelmap_file, 'r')\n",
    "labelmap = caffe_pb2.LabelMap()\n",
    "text_format.Merge(str(file.read()), labelmap)\n",
    "\n",
    "def get_labelname(labelmap, labels):\n",
    "    num_labels = len(labelmap.item)\n",
    "    labelnames = []\n",
    "    if type(labels) is not list:\n",
    "        labels = [labels]\n",
    "    for label in labels:\n",
    "        found = False\n",
    "        for i in xrange(0, num_labels):\n",
    "            if label == labelmap.item[i].label:\n",
    "                found = True\n",
    "                labelnames.append(labelmap.item[i].display_name)\n",
    "                break\n",
    "        assert found == True\n",
    "    return labelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmodel():\n",
    "    if IMAGE_SIZE == 300 and MODEL == 'coco':\n",
    "        model = 'deploy300.prototxt'\n",
    "        weights = 'VGG_coco_SSD_300x300_iter_400000.caffemodel'\n",
    "    elif IMAGE_SIZE == 512 and MODEL == 'coco':\n",
    "        model = 'deploy512.prototxt'\n",
    "        weights = 'VGG_coco_SSD_512x512_iter_360000.caffemodel'\n",
    "    else:\n",
    "        model = 'deploy2017.prototxt'\n",
    "        weights = 'VGG_coco_SSD_300x300_iter_184000.caffemodel'\n",
    "    return caffe.Net(model, weights, caffe.TEST) #how you load a model with weights in Caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "def preprocess(frame):\n",
    "    # Frame must be IMG_SIZExIMG_SIZEx3\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(frame)\n",
    "    channels[0] = clahe.apply(channels[0])\n",
    "    cv2.merge(channels,frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_YCR_CB2RGB)\n",
    "    frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_LANCZOS4)\n",
    "    # Frame must then be 3xHxW\n",
    "    if len(frame.shape) == 3:\n",
    "        frame = frame.transpose((2,0,1))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(image, net):    # (Batch size, channels, Image size, Image size) \n",
    "    # I wonder if we can increase the batch size and \n",
    "    # put a list of images together, but I guess that's more for training\n",
    "    net.blobs['data'].reshape(1,3,IMAGE_SIZE, IMAGE_SIZE)\n",
    "    # Transform the image to 1x3xSxS\n",
    "    net.blobs['data'].data[0,...] = image\n",
    "    # See ssd_detect.ipynb from Wei Liu, author of SSD\n",
    "    # https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_detect.py\n",
    "    detections = net.forward()['detection_out']\n",
    "    # Parse the output tensors\n",
    "    det_label = detections[0,0,:,1]\n",
    "    \n",
    "    det_conf = detections[0,0,:,2] #confidence\n",
    "    det_xmin = detections[0,0,:,3] #for bounding boxes per frame\n",
    "    det_ymin = detections[0,0,:,4]\n",
    "    det_xmax = detections[0,0,:,5]\n",
    "    det_ymax = detections[0,0,:,6]\n",
    "\n",
    "    # Keep only indices of detections with confidence higher than THRESHOLD\n",
    "    # in ssd_detect they keep it at 0.6, but that would be a confidence \n",
    "    # from the smaller set of PASCAL VOC cetegories. Coco has many more categories\n",
    "    # So a lower confidence still means a decent probability over the other categories\n",
    "    top_indices = [i for i, conf in enumerate(det_conf)] # take all detections here\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_labels = get_labelname(labelmap, top_label_indices)\n",
    "    \n",
    "    return (det_xmin, det_ymin, det_xmax, det_ymax, det_conf, top_labels, top_label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcDist(coords1, coords2):\n",
    "    return np.linalg.norm(coords1-coords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadvideo(filename, net):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    # Actually doesn't store real frames, but the frame shape of midpoint vectors\n",
    "    saved_frames = []\n",
    "    real_frames = []\n",
    "    FUZZY_MATCH = 10\n",
    "    FRAMES_TO_HOLD = 10\n",
    "    OWNER_DISTANCE = 50\n",
    "    frame_id = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            bag_updated = []\n",
    "            person_updated = []\n",
    "            frame_processed = preprocess(frame)\n",
    "            real_frames.append(frame)\n",
    "            processed_det = detect(frame_processed, net)\n",
    "            top_xmin, top_ymin, top_xmax, top_ymax, top_conf, top_labels, top_label_indices = processed_det\n",
    "            # Midpoint_boxes is a tensor, which has the area of the frame from the video\n",
    "            # But the value at each pixels position is only valid when it represents the midpoint of a detected box\n",
    "            # The values will be width, height, label, and \"owner_y, owner_x\" which is set to the coordinates\n",
    "            # of the person who is first within the threshold of what we consider owner if label is a bag\n",
    "            midpoint_boxes = np.empty((frame.shape[0], frame.shape[1], 5))\n",
    "            midpoint_boxes.fill(np.nan)\n",
    "            for i in range(top_conf.shape[0]):\n",
    "                xmin = int(round(top_xmin[i] * frame.shape[1]))\n",
    "                ymin = int(round(top_ymin[i] * frame.shape[0]))\n",
    "                xmax = int(round(top_xmax[i] * frame.shape[1]))\n",
    "                ymax = int(round(top_ymax[i] * frame.shape[0]))\n",
    "                score = top_conf[i]\n",
    "                label = int(top_label_indices[i])\n",
    "                label_name = top_labels[i]\n",
    "                display_txt = '%s: %.2f' % (label_name, score)\n",
    "                width = xmax - xmin + 1\n",
    "                height = ymax - ymin + 1\n",
    "                midx = xmin + (width) / 2\n",
    "                midy = ymin + (height) / 2\n",
    "                if label in [1, 4, 5, 6] and score > 0.1:\n",
    "                    obj_array = np.asarray(\n",
    "                        [height, width, label, np.nan, np.nan])\n",
    "                    midpoint_boxes[midy, midx] = obj_array\n",
    "                    found = -1\n",
    "                    if len(saved_frames) >= 1:\n",
    "                        for j in range(len(saved_frames) - 1, -1, -1):\n",
    "                            fuzzy_min = midy - FUZZY_MATCH if midy >= FUZZY_MATCH else 0\n",
    "                            fuzzx_min = midx - FUZZY_MATCH if midx >= FUZZY_MATCH else 0\n",
    "                            fuzzy_max = midy + FUZZY_MATCH if midy + \\\n",
    "                                FUZZY_MATCH < frame.shape[1] else frame.shape[1] - 1\n",
    "                            fuzzx_max = midx + FUZZY_MATCH if midx + \\\n",
    "                                FUZZY_MATCH < frame.shape[0] else frame.shape[0] - 1\n",
    "\n",
    "                            sub_sample = saved_frames[j][fuzzy_min:fuzzy_max,\n",
    "                                                         fuzzx_min:fuzzx_max]\n",
    "                            for row in range(sub_sample.shape[0]):\n",
    "                                box = sub_sample[row]\n",
    "                                if np.isfinite(box.flatten()).any():\n",
    "                                    for col in range(box.shape[0]):\n",
    "                                        # Previous some-odd frame at position [j][row, col]\n",
    "                                        pixel_midpoint = box[col]\n",
    "                                        # If person or object is ocluded match width _OR_ height being similar\n",
    "                                        if obj_array[0] - FUZZY_MATCH < pixel_midpoint[0] < obj_array[0] + FUZZY_MATCH or obj_array[1] - FUZZY_MATCH < pixel_midpoint[1] < obj_array[1] + FUZZY_MATCH and obj_array[2] == pixel_midpoint[2]:\n",
    "                                            midpoint_boxes[midy,\n",
    "                                                           midx][3:5] = pixel_midpoint[3:5]\n",
    "                                            if label == 1:\n",
    "                                                item_type = 1\n",
    "                                                person_updated.append(\n",
    "                                                    (row, col, midy, midx, pixel_midpoint[3], pixel_midpoint[4]))\n",
    "                                            # prev location, new loc\n",
    "                                            else:\n",
    "                                                bag_updated.append(\n",
    "                                                    (row, col, midy, midx, pixel_midpoint[3], pixel_midpoint[4]))\n",
    "                                            # For person/bag row/col means the place the object previously was\n",
    "                                            found = (j, row, col, midy,\n",
    "                                                     midx, label, pixel_midpoint[3], pixel_midpoint[3])\n",
    "                                            break\n",
    "                                if found != -1:\n",
    "                                    break\n",
    "                            if found != -1:\n",
    "                                break\n",
    "                        # Currently have in found the layer where the bag or person was last seen\n",
    "                        # found a person, check if person has moved and see if bag has also been identified\n",
    "                        # found a person, see if the bag was already found in this frame\n",
    "                        if found != -1 and found[5] == 1:\n",
    "                            for bag in bag_updated:\n",
    "                                if bag[0] == found[6] and bag[1] == found[7]:\n",
    "                                    # Bag's old owner position was this old owner's position\n",
    "                                    midpoint_boxes[bag[2],\n",
    "                                                   bag[3], 3:5] = found[3:5]  # Now new owner's position is held by bag\n",
    "                        elif found != -1:  # must be a bag that we found in this frame, see if the owner was updated\n",
    "                            for person in person_updated:\n",
    "                                if person[0] == found[6] and person[1] == found[7]:\n",
    "                                    # Bag's old owner position was this old owner's position\n",
    "                                    midpoint_boxes[person[2],\n",
    "                                                   person[3], 3:5] = found[3:5]  # Now new owner's position is held by bag\n",
    "                        if found != -1 and found[0] < FRAMES_TO_HOLD - 2:\n",
    "                            # Must have skipped a frame so add in relevant middle position\n",
    "                            missing_frames = FRAMES_TO_HOLD - found[0]\n",
    "                            diff_rows = found[3] - found[1]\n",
    "                            diff_cols = found[4] - found[2]\n",
    "                            # May be -b, or 0 , or +a\n",
    "                            incr_rows_per_frame = diff_rows // missing_frames\n",
    "                            incr_cols_per_frame = diff_cols // missing_frames\n",
    "                            for makeup_i in range(found[0], len(saved_frames)):\n",
    "                                saved_frames[makeup_i][found[1], found[2]] = saved_frames[found[0]][found[1], found[2]]\n",
    "                        if found == -1:\n",
    "                            # First time seeing the object, add\n",
    "                            if label == 1:  # First time seeing person\n",
    "                                person_updated.append(\n",
    "                                    (np.nan, np.nan, midy, midx, np.nan, np.nan))\n",
    "                            else:\n",
    "                                bag_updated.append(\n",
    "                                    (np.nan, np.nan, midy, midx, np.nan, np.nan))\n",
    "            if len(saved_frames) == 0:\n",
    "                # Do initial attribution of owners\n",
    "                for i in range(frame.shape[0]):\n",
    "                    for j in range(frame.shape[1]):\n",
    "                        if not np.isnan(midpoint_boxes[i, j, 0]) and midpoint_boxes[i, j, 2] in [4,5,6]:\n",
    "                            min_i = i - \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    i - OWNER_DISTANCE) > 0 else 0\n",
    "                            max_i = i + \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    i + OWNER_DISTANCE) < frame.shape[0] else frame.shape[0]\n",
    "                            min_j = j - \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    j - OWNER_DISTANCE) > 0 else 0\n",
    "                            max_j = j + \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    j + OWNER_DISTANCE) < frame.shape[1] else frame.shape[1] - 1\n",
    "                            found_owner = false\n",
    "                            potential_owners = []\n",
    "                            bag_coord = np.asarray([i, j])\n",
    "                            for y in range(min_i, max_i):\n",
    "                                for x in range(min_j, max_j):\n",
    "                                    if not np.isnan(midpoint_boxes[y, x, 0]) and midpoint_boxes[y, x, 2] == 1:\n",
    "                                        # y,x may be owner\n",
    "                                        potential_owners.append(\n",
    "                                            (y, x, calcDist(np.asarray([y, x]), bag_coord)))\n",
    "                            potential_owners = sorted(\n",
    "                                potential_owners, cmp=lambda a, b: int(a[2] - b[2]))\n",
    "                            if len(potential_owners) > 0:\n",
    "                                midpoint_boxes[i,j,3:5] = potential_owners[0][0:2]\n",
    "            for bag in bag_updated:\n",
    "                if np.isnan(bag[0]):\n",
    "                    # new bag not seen before\n",
    "                    bag_coord = np.asarray([bag[2], bag[3]])\n",
    "                    min_i = bag[2] - \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[2] - OWNER_DISTANCE) > 0 else 0\n",
    "                    max_i = bag[2] + \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[2] + OWNER_DISTANCE) < frame.shape[0] else frame.shape[0]\n",
    "                    min_j = bag[3] - \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[3] - OWNER_DISTANCE) > 0 else 0\n",
    "                    max_j = bag[3] + \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[3] + OWNER_DISTANCE) < frame.shape[1] else frame.shape[1] - 1\n",
    "                    potential_owners = []\n",
    "                    for i in range(min_i, max_i):\n",
    "                        for j in range(min_j, max_j):\n",
    "                            # look for person\n",
    "                            if midpoint_boxes[i, j, 2] == 1:\n",
    "                                # not a nan item, and is a person\n",
    "                                potential_owners.append((i, j, calcDist(\n",
    "                                    np.asarray([i, j]), bag_coord)))\n",
    "                    potential_owners = sorted(\n",
    "                        potential_owners, cmp=lambda a, b: int(a[2] - b[2]))\n",
    "                    if len(potential_owners) > 0:\n",
    "                        midpoint_boxes[bag[2],bag[3],3:5] = potential_owners[0][0:2]\n",
    "            saved_frames.append(midpoint_boxes)\n",
    "            if len(saved_frames) > FRAMES_TO_HOLD:\n",
    "                saved_first = saved_frames[0]\n",
    "                real_first = real_frames[0]\n",
    "                real_first = cv2.cvtColor(real_first, cv2.COLOR_BGR2RGB)\n",
    "                plt.rcParams['figure.figsize'] = (20, 20)\n",
    "                plt.tight_layout()\n",
    "                plt.imshow(real_first)\n",
    "                currentAxis = plt.gca()                \n",
    "                saved_frames = saved_frames[1:]\n",
    "                real_frames = real_frames[1:]\n",
    "                frame_id += 1\n",
    "                for i in range(saved_first.shape[0]):\n",
    "                    for j in range(saved_first.shape[1]):\n",
    "                        if not np.isnan(saved_first[i,j,0]) and saved_first[i,j,2] in [4,5,6]:\n",
    "                            # This is a bag and should have also had owner marked\n",
    "                            bag = saved_first[i,j]\n",
    "                            bag_y = i - bag[0]//2\n",
    "                            bag_x = j - bag[1]//2\n",
    "                            bag_coord = (bag_x, bag_y), bag[1], bag[0]                \n",
    "                            color = '#FF5555'\n",
    "                            if not np.isnan(bag[3]) and not np.isnan(saved_first[int(bag[3]), int(bag[4])][0]):\n",
    "                                # has owner\n",
    "                                color = '#5555FF'\n",
    "                                print(bag[3:5])\n",
    "                                owner = saved_first[int(bag[3]), int(bag[4])]\n",
    "                                print(owner)\n",
    "                                owner_y = bag[3] - owner[0]//2\n",
    "                                owner_x = bag[4] - owner[1]//2\n",
    "                                owner_coord = (owner_x, owner_y), owner[1], owner[0]\n",
    "                                currentAxis.add_patch(plt.Rectangle(*owner_coord, fill=False, edgecolor=color, linewidth=4))\n",
    "                                currentAxis.text(owner_x, owner_y, \"owner\", bbox={'facecolor':color, 'alpha':0.5})\n",
    "                            currentAxis.add_patch(plt.Rectangle(*bag_coord, fill=False, edgecolor=color, linewidth=3))\n",
    "                            currentAxis.text(bag_x, bag_y, \"luggage\", bbox={'facecolor':color, 'alpha':0.5})\n",
    "                figure_name = 'images-numpy/%s_%05d.jpg' %('image', frame_id)\n",
    "                plt.savefig(figure_name)\n",
    "                plt.clf()\n",
    "        else:\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print 'how did we break?'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 369.  271.]\n",
      "[ 356.  118.    1.   nan   nan]\n",
      "[ 340.  289.]\n",
      "[ 338.  116.    1.   nan   nan]\n",
      "[ 348.  176.]\n",
      "[ 323.  118.    1.   nan   nan]\n",
      "[ 336.  181.]\n",
      "[ 307.  115.    1.   nan   nan]\n",
      "[ 331.  305.]\n",
      "[ 321.  102.    1.   nan   nan]\n",
      "[ 323.  306.]\n",
      "[ 314.  102.    1.   nan   nan]\n",
      "[ 331.  190.]\n",
      "[ 295.  118.    1.   nan   nan]\n",
      "[ 330.  194.]\n",
      "[ 295.  112.    1.   nan   nan]\n",
      "[ 110.  580.]\n",
      "[ 86.  23.   1.  nan  nan]\n",
      "[ 216.  336.]\n",
      "[ 115.  381.    1.   nan   nan]\n",
      "[ 216.  336.]\n",
      "[ 115.  381.    1.   nan   nan]\n",
      "[ 325.  200.]\n",
      "[ 296.  114.    1.   nan   nan]\n",
      "[ 298.  314.]\n",
      "[ 286.   98.    1.   nan   nan]\n",
      "[ 298.  314.]\n",
      "[ 286.   98.    1.   nan   nan]\n",
      "[ 302.  316.]\n",
      "[ 291.   99.    1.   nan   nan]\n",
      "[ 129.  567.]\n",
      "[ 136.   40.    1.   nan   nan]\n",
      "[ 304.  210.]\n",
      "[ 277.  122.    1.   nan   nan]\n",
      "[ 301.  319.]\n",
      "[ 284.   96.    1.   nan   nan]\n",
      "[ 137.  590.]\n",
      "[  66.  146.    1.   nan   nan]\n",
      "[ 215.  335.]\n",
      "[ 116.  368.    1.   nan   nan]\n",
      "[ 305.  213.]\n",
      "[ 281.  116.    1.   nan   nan]\n",
      "[ 284.  358.]\n",
      "[ 423.  562.    1.   nan   nan]\n",
      "[ 213.  333.]\n",
      "[ 117.  370.    1.   nan   nan]\n",
      "[ 284.  358.]\n",
      "[ 425.  562.    1.   nan   nan]\n",
      "[ 213.  330.]\n",
      "[ 118.  366.    1.   nan   nan]\n",
      "[ 213.  331.]\n",
      "[ 117.  371.    1.   nan   nan]\n",
      "[ 286.  356.]\n",
      "[ 425.  561.    1.   nan   nan]\n",
      "[ 276.  328.]\n",
      "[ 273.   94.    1.   nan   nan]\n",
      "[ 137.  580.]\n",
      "[  69.  143.    1.   nan   nan]\n",
      "[ 276.  225.]\n",
      "[ 259.   99.    1.   nan   nan]\n",
      "[ 152.  585.]\n",
      "[ 155.  134.    1.   nan   nan]\n",
      "[ 152.  585.]\n",
      "[ 155.  134.    1.   nan   nan]\n",
      "[ 212.  328.]\n",
      "[ 118.  379.    1.   nan   nan]\n",
      "[ 275.  228.]\n",
      "[ 256.   96.    1.   nan   nan]\n",
      "[ 150.  531.]\n",
      "[ 155.  144.    1.   nan   nan]\n",
      "[ 171.  561.]\n",
      "[ 172.   58.    1.   nan   nan]\n",
      "[ 171.  561.]\n",
      "[ 172.   58.    1.   nan   nan]\n",
      "[ 172.  556.]\n",
      "[ 177.   61.    1.   nan   nan]\n",
      "[ 172.  556.]\n",
      "[ 176.   63.    1.   nan   nan]\n",
      "[ 161.  526.]\n",
      "[ 172.   50.    1.   nan   nan]\n",
      "[ 237.  286.]\n",
      "[ 189.   57.    1.   nan   nan]\n",
      "[ 237.  286.]\n",
      "[ 189.   57.    1.   nan   nan]\n",
      "[ 236.  338.]\n",
      "[ 233.   80.    1.   nan   nan]\n",
      "[ 174.  536.]\n",
      "[ 179.   60.    1.   nan   nan]\n",
      "[ 186.  255.]\n",
      "[ 90.  71.   1.  nan  nan]\n",
      "[ 174.  536.]\n",
      "[ 180.   61.    1.   nan   nan]\n",
      "[ 286.  345.]\n",
      "[ 427.  559.    1.   nan   nan]\n",
      "[ 235.  338.]\n",
      "[ 239.   77.    1.   nan   nan]\n",
      "[ 173.  258.]\n",
      "[ 89.  74.   1.  nan  nan]\n",
      "[ 223.  337.]\n",
      "[ 231.   75.    1.   nan   nan]\n",
      "[ 179.  537.]\n",
      "[ 174.   53.    1.   nan   nan]\n",
      "[ 285.  351.]\n",
      "[ 422.  561.    1.   nan   nan]\n",
      "[ 218.  337.]\n",
      "[ 230.   76.    1.   nan   nan]\n",
      "[ 160.  266.]\n",
      "[ 85.  64.   1.  nan  nan]\n",
      "[ 217.  337.]\n",
      "[ 231.   75.    1.   nan   nan]\n",
      "[ 213.  344.]\n",
      "[ 222.   72.    1.   nan   nan]\n",
      "[ 163.  276.]\n",
      "[ 94.  69.   1.  nan  nan]\n",
      "[ 200.  350.]\n",
      "[ 200.   72.    1.   nan   nan]\n",
      "[ 191.  545.]\n",
      "[ 153.   75.    1.   nan   nan]\n",
      "[ 146.  355.]\n",
      "[ 99.  59.   1.  nan  nan]\n",
      "[ 194.  351.]\n",
      "[ 204.   69.    1.   nan   nan]\n",
      "[ 194.  351.]\n",
      "[ 204.   69.    1.   nan   nan]\n",
      "[ 192.  355.]\n",
      "[ 200.   71.    1.   nan   nan]\n",
      "[ 188.  362.]\n",
      "[ 194.   68.    1.   nan   nan]\n",
      "[ 189.  368.]\n",
      "[ 183.   67.    1.   nan   nan]\n",
      "[ 197.  551.]\n",
      "[ 152.   67.    1.   nan   nan]\n",
      "[ 194.  312.]\n",
      "[ 181.   72.    1.   nan   nan]\n",
      "[ 185.  375.]\n",
      "[ 195.   69.    1.   nan   nan]\n",
      "[ 185.  375.]\n",
      "[ 195.   69.    1.   nan   nan]\n",
      "[ 185.  375.]\n",
      "[ 195.   69.    1.   nan   nan]\n",
      "[ 196.  490.]\n",
      "[ 162.   70.    1.   nan   nan]\n",
      "[ 185.  320.]\n",
      "[ 170.   64.    1.   nan   nan]\n",
      "[ 191.  490.]\n",
      "[ 191.   74.    1.   nan   nan]\n",
      "[ 197.  488.]\n",
      "[ 200.   78.    1.   nan   nan]\n",
      "[ 181.  338.]\n",
      "[ 171.   66.    1.   nan   nan]\n",
      "[ 199.  469.]\n",
      "[ 187.   73.    1.   nan   nan]\n",
      "[ 144.  453.]\n",
      "[ 88.  68.   1.  nan  nan]\n",
      "[ 202.  451.]\n",
      "[ 205.   84.    1.   nan   nan]\n",
      "[ 178.  370.]\n",
      "[ 171.   60.    1.   nan   nan]\n",
      "[ 206.  436.]\n",
      "[ 207.  176.    1.   nan   nan]\n",
      "[ 215.  443.]\n",
      "[ 210.   81.    1.   nan   nan]\n",
      "[ 200.  476.]\n",
      "[ 159.   66.    1.   nan   nan]\n",
      "[ 219.  435.]\n",
      "[ 230.   76.    1.   nan   nan]\n",
      "[ 220.  434.]\n",
      "[ 207.   88.    1.   nan   nan]\n",
      "[ 163.  569.]\n",
      "[ 173.   47.    1.   nan   nan]\n",
      "[ 237.  410.]\n",
      "[ 236.  106.    1.   nan   nan]\n",
      "[ 256.  277.]\n",
      "[ 249.   97.    1.   nan   nan]\n",
      "[ 257.  257.]\n",
      "[ 243.  108.    1.   nan   nan]\n",
      "[ 265.  174.]\n",
      "[ 248.  111.    1.   nan   nan]\n",
      "[ 271.  116.]\n",
      "[ 250.  127.    1.   nan   nan]\n",
      "[ 265.  105.]\n",
      "[ 249.  107.    1.   nan   nan]\n",
      "[ 261.   74.]\n",
      "[ 271.  108.    1.   nan   nan]\n",
      "[ 175.  571.]\n",
      "[ 175.   42.    1.   nan   nan]\n",
      "[ 175.  571.]\n",
      "[ 175.   42.    1.   nan   nan]\n",
      "[ 175.  569.]\n",
      "[ 180.   41.    1.   nan   nan]\n",
      "[ 100.  420.]\n",
      "[ 124.  384.    1.   nan   nan]\n",
      "[ 116.  400.]\n",
      "[ 125.   41.    1.   nan   nan]\n"
     ]
    }
   ],
   "source": [
    "net = loadmodel()\n",
    "loadvideo('AVSS_AB_Easy_Clipped.mov', net)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer outputs the images here, but they are all in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
