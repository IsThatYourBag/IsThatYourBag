{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mat\n",
    "import matplotlib.pyplot as plt\n",
    "import caffe\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "#MODEL = 'ILSVRC' # ImageNet, don't use ImageNet, it wasn't trained on all categories\n",
    "#MODEL = 'coco' # MS-Coco\n",
    "MODEL = 'ours'\n",
    "IMAGE_SIZE = 300 # 300x300 trained on coco or ILSVRC \n",
    "# I wonder if we can take the coco model and further train it on\n",
    "# http://image-net.org/synset?wnid=n02773838\n",
    "#IMAGE_SIZE = 512 # for 512x512 trained on coco\n",
    "THRESHOLD = 0.20 # for detection - percentage that the model is sure it's what you're looking for\n",
    "# There are 21 categories.... pick one color for each\n",
    "# just a tool for label finding\n",
    "any_in = lambda a, b: bool(set(a).intersection(b)) #for checking if a list contains elements of another\n",
    "COLORS = plt.cm.hsv(np.linspace(0, 1, 255)).tolist() #for picking colors of the boxes\n",
    "caffe.set_device(0)\n",
    "caffe.set_mode_gpu()\n",
    "#caffe.set_mode_cpu()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "from caffe.proto import caffe_pb2\n",
    "\n",
    "# load COCO labels\n",
    "if MODEL == 'ours':\n",
    "    labelmap_file = 'data/coco/labelmap_coco.prototxt'\n",
    "if MODEL == 'coco':\n",
    "    labelmap_file = 'data/coco/labelmap_coco2.prototxt'\n",
    "else:\n",
    "    labelmap_file = 'data/ILSVRC2016/labelmap_ilsvrc_det.prototxt'\n",
    "file = open(labelmap_file, 'r')\n",
    "labelmap = caffe_pb2.LabelMap()\n",
    "text_format.Merge(str(file.read()), labelmap)\n",
    "\n",
    "def get_labelname(labelmap, labels):\n",
    "    num_labels = len(labelmap.item)\n",
    "    labelnames = []\n",
    "    if type(labels) is not list:\n",
    "        labels = [labels]\n",
    "    for label in labels:\n",
    "        found = False\n",
    "        for i in xrange(0, num_labels):\n",
    "            if label == labelmap.item[i].label:\n",
    "                found = True\n",
    "                labelnames.append(labelmap.item[i].display_name)\n",
    "                break\n",
    "        assert found == True\n",
    "    return labelnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmodel():\n",
    "    if IMAGE_SIZE == 300 and MODEL == 'coco':\n",
    "        model = 'deploy300.prototxt'\n",
    "        weights = 'VGG_coco_SSD_300x300_iter_400000.caffemodel'\n",
    "    elif IMAGE_SIZE == 512 and MODEL == 'coco':\n",
    "        model = 'deploy512.prototxt'\n",
    "        weights = 'VGG_coco_SSD_512x512_iter_360000.caffemodel'\n",
    "    else:\n",
    "        model = 'deploy2017.prototxt'\n",
    "        weights = 'VGG_coco_SSD_300x300_iter_184000.caffemodel'\n",
    "    return caffe.Net(model, weights, caffe.TEST) #how you load a model with weights in Caffe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "def preprocess(frame):\n",
    "    # Frame must be IMG_SIZExIMG_SIZEx3\n",
    "    frame = cv2.cvtColor(frame,cv2.COLOR_BGR2YCR_CB)\n",
    "    channels = cv2.split(frame)\n",
    "    channels[0] = clahe.apply(channels[0])\n",
    "    cv2.merge(channels,frame)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_YCR_CB2RGB)\n",
    "    frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_LANCZOS4)\n",
    "    # Frame must then be 3xHxW\n",
    "    if len(frame.shape) == 3:\n",
    "        frame = frame.transpose((2,0,1))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect(image, net):    # (Batch size, channels, Image size, Image size) \n",
    "    # I wonder if we can increase the batch size and \n",
    "    # put a list of images together, but I guess that's more for training\n",
    "    net.blobs['data'].reshape(1,3,IMAGE_SIZE, IMAGE_SIZE)\n",
    "    # Transform the image to 1x3xSxS\n",
    "    net.blobs['data'].data[0,...] = image\n",
    "    # See ssd_detect.ipynb from Wei Liu, author of SSD\n",
    "    # https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_detect.py\n",
    "    detections = net.forward()['detection_out']\n",
    "    # Parse the output tensors\n",
    "    det_label = detections[0,0,:,1]\n",
    "    \n",
    "    det_conf = detections[0,0,:,2] #confidence\n",
    "    det_xmin = detections[0,0,:,3] #for bounding boxes per frame\n",
    "    det_ymin = detections[0,0,:,4]\n",
    "    det_xmax = detections[0,0,:,5]\n",
    "    det_ymax = detections[0,0,:,6]\n",
    "\n",
    "    # Keep only indices of detections with confidence higher than THRESHOLD\n",
    "    # in ssd_detect they keep it at 0.6, but that would be a confidence \n",
    "    # from the smaller set of PASCAL VOC cetegories. Coco has many more categories\n",
    "    # So a lower confidence still means a decent probability over the other categories\n",
    "    top_indices = [i for i, conf in enumerate(det_conf)] # take all detections here\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_labels = get_labelname(labelmap, top_label_indices)\n",
    "    \n",
    "    return (det_xmin, det_ymin, det_xmax, det_ymax, det_conf, top_labels, top_label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcDist(coords1, coords2):\n",
    "    return np.linalg.norm(coords1-coords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadvideo(filename, net):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    # Actually doesn't store real frames, but the frame shape of midpoint vectors\n",
    "    saved_frames = []\n",
    "    FUZZY_MATCH = 10\n",
    "    FRAMES_TO_HOLD = 10\n",
    "    OWNER_DISTANCE = 50\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            bag_updated = []\n",
    "            person_updated = []\n",
    "            frame_processed = preprocess(frame)\n",
    "            processed_det = detect(frame_processed, net)\n",
    "            top_xmin, top_ymin, top_xmax, top_ymax, top_conf, top_labels, top_label_indices = processed_det\n",
    "            # Midpoint_boxes is a tensor, which has the area of the frame from the video\n",
    "            # But the value at each pixels position is only valid when it represents the midpoint of a detected box\n",
    "            # The values will be width, height, label, and \"owner_y, owner_x\" which is set to the coordinates\n",
    "            # of the person who is first within the threshold of what we consider owner if label is a bag\n",
    "            midpoint_boxes = np.empty((frame.shape[0], frame.shape[1], 5))\n",
    "            midpoint_boxes.fill(np.nan)\n",
    "            for i in range(top_conf.shape[0]):\n",
    "                xmin = int(round(top_xmin[i] * frame.shape[1]))\n",
    "                ymin = int(round(top_ymin[i] * frame.shape[0]))\n",
    "                xmax = int(round(top_xmax[i] * frame.shape[1]))\n",
    "                ymax = int(round(top_ymax[i] * frame.shape[0]))\n",
    "                score = top_conf[i]\n",
    "                label = int(top_label_indices[i])\n",
    "                label_name = top_labels[i]\n",
    "                display_txt = '%s: %.2f' % (label_name, score)\n",
    "                width = xmax - xmin + 1\n",
    "                height = ymax - ymin + 1\n",
    "                midx = xmin + (width) / 2\n",
    "                midy = ymin + (height) / 2\n",
    "                if label in [1, 4, 5, 6] and score > 0.1:\n",
    "                    obj_array = np.asarray(\n",
    "                        [width, height, label, np.nan, np.nan])\n",
    "                    midpoint_boxes[midy, midx] = obj_array\n",
    "                    found = -1\n",
    "                    if len(saved_frames) >= 1:\n",
    "                        for j in range(len(saved_frames) - 1, -1, -1):\n",
    "                            fuzzy_min = midy - FUZZY_MATCH if midy >= FUZZY_MATCH else 0\n",
    "                            fuzzx_min = midx - FUZZY_MATCH if midx >= FUZZY_MATCH else 0\n",
    "                            fuzzy_max = midy + FUZZY_MATCH if midy + \\\n",
    "                                FUZZY_MATCH < frame.shape[1] else frame.shape[1] - 1\n",
    "                            fuzzx_max = midx + FUZZY_MATCH if midx + \\\n",
    "                                FUZZY_MATCH < frame.shape[0] else frame.shape[0] - 1\n",
    "\n",
    "                            sub_sample = saved_frames[j][fuzzy_min:fuzzy_max,\n",
    "                                                         fuzzx_min:fuzzx_max]\n",
    "                            for row in range(sub_sample.shape[0]):\n",
    "                                box = sub_sample[row]\n",
    "                                if np.isfinite(box.flatten()).any():\n",
    "                                    for col in range(box.shape[0]):\n",
    "                                        # Previous some-odd frame at position [j][row, col]\n",
    "                                        pixel_midpoint = box[col]\n",
    "                                        # If person or object is ocluded match width _OR_ height being similar\n",
    "                                        if obj_array[0] - FUZZY_MATCH < pixel_midpoint[0] < obj_array[0] + FUZZY_MATCH or obj_array[1] - FUZZY_MATCH < pixel_midpoint[1] < obj_array[1] + FUZZY_MATCH and obj_array[2] == pixel_midpoint[2]:\n",
    "                                            midpoint_boxes[midy,\n",
    "                                                           midx][3:5] = pixel_midpoint[3:5]\n",
    "                                            if label == 1:\n",
    "                                                item_type = 1\n",
    "                                                person_updated.append(\n",
    "                                                    (row, col, midy, midx, pixel_midpoint[3], pixel_midpoint[4]))\n",
    "                                            # prev location, new loc\n",
    "                                            else:\n",
    "                                                bag_updated.append(\n",
    "                                                    (row, col, midy, midx, pixel_midpoint[3], pixel_midpoint[4]))\n",
    "                                            # For person/bag row/col means the place the object previously was\n",
    "                                            found = (j, row, col, midy,\n",
    "                                                     midx, label, pixel_midpoint[3], pixel_midpoint[3])\n",
    "                                            break\n",
    "                                if found != -1:\n",
    "                                    break\n",
    "                            if found != -1:\n",
    "                                break\n",
    "                        # Currently have in found the layer where the bag or person was last seen\n",
    "                        # found a person, check if person has moved and see if bag has also been identified\n",
    "                        # found a person, see if the bag was already found in this frame\n",
    "                        if found != -1 and found[5] == 1:\n",
    "                            for bag in bag_updated:\n",
    "                                if bag[0] == found[6] and bag[1] == found[7]:\n",
    "                                    # Bag's old owner position was this old owner's position\n",
    "                                    midpoint_boxes[bag[2],\n",
    "                                                   bag[3], 3:5] = found[3:5]  # Now new owner's position is held by bag\n",
    "                        elif found != -1:  # must be a bag that we found in this frame, see if the owner was updated\n",
    "                            for person in person_updated:\n",
    "                                if person[0] == found[6] and person[1] == found[7]:\n",
    "                                    # Bag's old owner position was this old owner's position\n",
    "                                    midpoint_boxes[person[2],\n",
    "                                                   person[3], 3:5] = found[3:5]  # Now new owner's position is held by bag\n",
    "                        if found != -1 and found[0] < FRAMES_TO_HOLD - 2:\n",
    "                            # Must have skipped a frame so add in relevant middle position\n",
    "                            missing_frames = FRAMES_TO_HOLD - found[0]\n",
    "                            diff_rows = found[3] - found[1]\n",
    "                            diff_cols = found[4] - found[2]\n",
    "                            # May be -b, or 0 , or +a\n",
    "                            incr_rows_per_frame = diff_rows // missing_frames\n",
    "                            incr_cols_per_frame = diff_cols // missing_frames\n",
    "                            for makeup_i in range(found[0], len(saved_frames)):\n",
    "                                saved_frames[makeup_i][found[1], found[2]] = saved_frames[found[0]][found[1], found[2]]\n",
    "                        if found == -1:\n",
    "                            # First time seeing the object, add\n",
    "                            if label == 1:  # First time seeing person\n",
    "                                person_updated.append(\n",
    "                                    (np.nan, np.nan, midy, midx, np.nan, np.nan))\n",
    "                            else:\n",
    "                                bag_updated.append(\n",
    "                                    (np.nan, np.nan, midy, midx, np.nan, np.nan))\n",
    "            if len(saved_frames) == 0:\n",
    "                # Do initial attribution of owners\n",
    "                for i in range(frame.shape[0]):\n",
    "                    for j in range(frame.shape[1]):\n",
    "                        if not np.isnan(midpoint_boxes[i, j, 0]) and midpoint_boxes[i, j, 2] in [4,5,6]:\n",
    "                            min_i = i - \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    i - OWNER_DISTANCE) > 0 else 0\n",
    "                            max_i = i + \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    i + OWNER_DISTANCE) < frame.shape[0] else frame.shape[0]\n",
    "                            min_j = j - \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    j - OWNER_DISTANCE) > 0 else 0\n",
    "                            max_j = j + \\\n",
    "                                OWNER_DISTANCE if (\n",
    "                                    j + OWNER_DISTANCE) < frame.shape[1] else frame.shape[1] - 1\n",
    "                            found_owner = false\n",
    "                            potential_owners = []\n",
    "                            bag_coord = np.asarray([i, j])\n",
    "                            for y in range(min_i, max_i):\n",
    "                                for x in range(min_j, max_j):\n",
    "                                    if not np.isnan(midpoint_boxes[y, x, 0]) and midpoint_boxes[y, x, 2] == 1:\n",
    "                                        # y,x may be owner\n",
    "                                        potential_owners.append(\n",
    "                                            (y, x, calcDist(np.asarray([y, x]), bag_coord)))\n",
    "                            potential_owners = sorted(\n",
    "                                potential_owners, cmp=lambda a, b: int(a[2] - b[2]))\n",
    "                            if len(potential_owners) > 0:\n",
    "                                midpoint_boxes[i,j,3:5] = potential_owners[0][0:2]\n",
    "            for bag in bag_updated:\n",
    "                if np.isnan(bag[0]):\n",
    "                    # new bag not seen before\n",
    "                    bag_coord = np.asarray([bag[2], bag[3]])\n",
    "                    min_i = bag[2] - \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[2] - OWNER_DISTANCE) > 0 else 0\n",
    "                    max_i = bag[2] + \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[2] + OWNER_DISTANCE) < frame.shape[0] else frame.shape[0]\n",
    "                    min_j = bag[3] - \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[3] - OWNER_DISTANCE) > 0 else 0\n",
    "                    max_j = bag[3] + \\\n",
    "                        OWNER_DISTANCE if (\n",
    "                            bag[3] + OWNER_DISTANCE) < frame.shape[1] else frame.shape[1] - 1\n",
    "                    potential_owners = []\n",
    "                    for i in range(min_i, max_i):\n",
    "                        for j in range(min_j, max_j):\n",
    "                            # look for person\n",
    "                            if midpoint_boxes[i, j, 2] == 1:\n",
    "                                # not a nan item, and is a person\n",
    "                                potential_owners.append((i, j, calcDist(\n",
    "                                    np.asarray([i, j]), bag_coord)))\n",
    "                    potential_owners = sorted(\n",
    "                        potential_owners, cmp=lambda a, b: int(a[2] - b[2]))\n",
    "                    if len(potential_owners) > 0:\n",
    "                        midpoint_boxes[bag[2],bag[3],3:5] = potential_owners[0][0:2]\n",
    "            saved_frames.append(midpoint_boxes)\n",
    "            if len(saved_frames) > FRAMES_TO_HOLD:\n",
    "                saved_frames = saved_frames[1:]\n",
    "        else:\n",
    "            break\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print 'how did we break?'\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[(369, 271, 56.0357029044876)]\n",
      "[]\n",
      "[(340, 289, 33.015148038438355)]\n",
      "[]\n",
      "[(348, 176, 48.846698967279252)]\n",
      "[(336, 181, 46.647615158762406)]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[(331, 305, 48.270073544588683)]\n",
      "[(323, 306, 51.224993899462788)]\n",
      "[(331, 190, 46.818799642878503)]\n",
      "[(330, 194, 47.539457296018853)]\n",
      "[(216, 336, 28.653097563788805)]\n",
      "[(216, 336, 46.043457732885351), (301, 313, 46.324939287601879)]\n",
      "[(325, 200, 58.796258384356399)]\n",
      "[(110, 580, 9.8488578017961039), (99, 570, 15.033296378372908), (128, 570, 14.035668847618199), (139, 617, 52.354560450833695)]\n",
      "[]\n",
      "[]\n",
      "[(298, 314, 29.154759474226502)]\n",
      "[(298, 314, 41.629316592997299), (298, 390, 41.629316592997299)]\n",
      "[(302, 316, 47.927027865287037), (216, 337, 58.0)]\n",
      "[(129, 567, 18.439088914585774), (118, 585, 30.14962686336267)]\n",
      "[(304, 210, 51.039200620699383)]\n",
      "[(301, 319, 31.89043743820395)]\n",
      "[]\n",
      "[(215, 335, 34.438350715445125), (291, 321, 53.535035257296691)]\n",
      "[(137, 590, 16.278820596099706), (118, 564, 18.867962264113206), (145, 589, 18.601075237738275), (114, 581, 21.189620100417091), (177, 558, 45.880278987817846)]\n",
      "[(284, 358, 12.041594578792296), (283, 323, 36.400549446402593)]\n",
      "[(305, 213, 56.859475903318)]\n",
      "[(213, 333, 17.4928556845359)]\n",
      "[(213, 330, 32.202484376209235), (276, 326, 53.254107822777392)]\n",
      "[(213, 331, 21.023796041628639), (274, 326, 40.19950248448356)]\n",
      "[(286, 356, 16.643316977093239), (274, 326, 48.754486972995622)]\n",
      "[(276, 328, 8.2462112512353212), (285, 356, 22.825424421026653)]\n",
      "[(276, 225, 41.048751503547585)]\n",
      "[(152, 585, 6.0), (138, 582, 20.223748416156685), (170, 562, 25.942243542145693), (119, 584, 39.012818406262319), (109, 583, 49.040799340956916), (110, 582, 48.093658625644196), (111, 567, 50.328918128646478)]\n",
      "[(152, 585, 18.027756377319946), (138, 582, 19.849433241279208), (170, 562, 19.646882704388499), (119, 584, 36.235341863986875), (142, 532, 36.138621999185304), (111, 567, 40.0), (129, 534, 39.66106403010388), (110, 582, 43.657759905886145), (109, 583, 44.944410108488462), (103, 586, 51.623637996561229), (106, 543, 51.0), (109, 531, 55.317266743757322)]\n",
      "[(275, 228, 38.600518131237564)]\n",
      "[(212, 328, 12.165525060596439), (188, 333, 23.086792761230392)]\n",
      "[(150, 531, 14.866068747318506), (144, 530, 18.601075237738275), (171, 561, 22.627416997969522), (123, 561, 35.777087639996637), (152, 584, 39.11521443121589), (138, 582, 40.718546143004666), (113, 547, 42.047592083257278), (169, 586, 43.324358044868937), (108, 529, 49.648766349225639), (107, 566, 52.392747589718944), (119, 583, 52.345009313209601), (111, 581, 56.850681614207581), (199, 510, 56.222771187482387), (111, 583, 58.137767414994535)]\n",
      "[(171, 561, 10.0), (169, 586, 21.470910553583888), (152, 584, 31.906112267087632), (138, 582, 43.657759905886145), (150, 531, 46.227697325304881), (144, 530, 50.931326312987373)]\n",
      "[]\n",
      "[(171, 561, 15.132745950421556), (150, 531, 27.459060435491963), (144, 530, 33.120990323358392), (169, 586, 40.19950248448356), (152, 584, 43.416586692184822), (199, 510, 44.407206622349037), (138, 582, 50.209560842532767), (123, 561, 52.201532544552748)]\n",
      "[(172, 556, 10.770329614269007), (148, 531, 31.76476034853718), (169, 586, 40.607881008493905), (155, 584, 43.416586692184822), (200, 511, 42.43819034784589), (138, 585, 54.451813560247928)]\n",
      "[]\n",
      "[(161, 526, 34.828149534535996), (171, 549, 37.013511046643494), (101, 526, 39.66106403010388)]\n",
      "[(237, 286, 36.235341863986875), (259, 249, 45.891175622335062)]\n",
      "[(237, 286, 15.264337522473747), (259, 249, 45.453272709454048)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fa7355e61c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloadvideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AVSS_AB_Easy_Clipped.mov'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-06c5cff9af33>\u001b[0m in \u001b[0;36mloadvideo\u001b[0;34m(filename, net)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mperson_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mframe_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mprocessed_det\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mtop_xmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ymin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_xmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ymax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_label_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_det\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# Midpoint_boxes is a tensor, which has the area of the frame from the video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-63988384c8c8>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(image, net)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# See ssd_detect.ipynb from Wei Liu, author of SSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_detect.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detection_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Parse the output tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdet_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/eli/Downloads/caffe/build/install/python/caffe/pycaffe.pyc\u001b[0m in \u001b[0;36m_Net_forward\u001b[0;34m(self, blobs, start, end, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;31m# Unpack blobs to extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = loadmodel()\n",
    "loadvideo('AVSS_AB_Easy_Clipped.mov', net)\n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer outputs the images here, but they are all in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
